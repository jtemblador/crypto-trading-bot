{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1820c42-73db-42b2-a63e-c09517abbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import stuff\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87c44c0f-1d80-4e9a-99c1-557f2b25fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_and_load_crypto(zip_folder_path):\n",
    "    all_crypto = []\n",
    "    zip_files = glob(os.path.join(zip_folder_path, '*.zip'))\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            extract_path = os.path.splitext(zip_file)[0]\n",
    "            os.makedirs(extract_path, exist_ok=True)\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        csv_files = glob(os.path.join(extract_path, '*.csv'))\n",
    "        for csv in csv_files:\n",
    "            df = pd.read_csv(csv)\n",
    "            all_crypto.append(df)\n",
    "\n",
    "    crypto_df = pd.concat(all_crypto, ignore_index=True)\n",
    "    return crypto_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "471e5365-fb1a-4424-8cf6-3e622cbc66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit_sentiment(sentiment_folder_path):\n",
    "    all_reddit = []\n",
    "    csv_files = glob(os.path.join(sentiment_folder_path, '*.csv'))\n",
    "\n",
    "    for csv in csv_files:\n",
    "        if os.path.isfile(csv):  # Fix: only open real files\n",
    "            df = pd.read_csv(csv)\n",
    "            all_reddit.append(df)\n",
    "\n",
    "    reddit_df = pd.concat(all_reddit, ignore_index=True)\n",
    "    return reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e882f15-c2ef-43a8-ac8e-0c3d4daa0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(crypto_df, reddit_df):\n",
    "    crypto_df['timestamp'] = pd.to_datetime(crypto_df['timestamp']) # Converts timestampl colums from string to datetime objs\n",
    "    reddit_df['timestamp'] = pd.to_datetime(reddit_df['timestamp'])\n",
    "\n",
    "    crypto_df.set_index('timestamp', inplace=True) # Set timestamp as the index of each DataFrame\n",
    "    reddit_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # THIS SETS HOUR INTERVALSA, MAY CHANGE\n",
    "    crypto_hourly = crypto_df['close'].resample('h').last()  # Hour intervals, for now\n",
    "    reddit_hourly = reddit_df['sentiment'].resample('h').mean()\n",
    "\n",
    "    # Shift the crypto closing prices 6 hours backwards, for every hour we now have price 6 hours into FUTURE\n",
    "    future_price = crypto_hourly.shift(-6)  # 6 hours into future\n",
    "\n",
    "    # This DA DATA, contains everything listed under me\n",
    "    data = pd.DataFrame({\n",
    "        'sentiment': reddit_hourly,\n",
    "        'close_price': crypto_hourly,\n",
    "        'future_close_price': future_price\n",
    "    })\n",
    "\n",
    "    data.dropna(inplace=True) # Drops any rows where data is missing, Can happen if at beggining or end, or if no reddit post synced with kraken data\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d4340d4-aa49-454d-974b-99d493fbd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(data, max_lag_hours=12):\n",
    "    for lag in range(1, max_lag_hours + 1):\n",
    "        data[f'sentiment_prev_{lag}h'] = data['sentiment'].shift(lag)\n",
    "        data[f'close_price_prev_{lag}h'] = data['close_price'].shift(lag)\n",
    "\n",
    "    # Create target: price movement 6 hours later\n",
    "    data['target'] = (data['future_close_price'] > data['close_price']).astype(int)\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02a47238-8d01-4de0-bd78-801d22808d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    feature_cols = [col for col in data.columns if 'sentiment' in col or 'close_price_prev' in col]\n",
    "    X = data[feature_cols]\n",
    "    y = data['target']\n",
    "\n",
    "    train_end = int(len(X) * 0.7)\n",
    "    valid_end = int(len(X) * 0.85)\n",
    "\n",
    "    X_train = X.iloc[:train_end]\n",
    "    y_train = y.iloc[:train_end]\n",
    "\n",
    "    X_valid = X.iloc[train_end:valid_end]\n",
    "    y_valid = y.iloc[train_end:valid_end]\n",
    "\n",
    "    X_test = X.iloc[valid_end:]\n",
    "    y_test = y.iloc[valid_end:]\n",
    "\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f532ad0-05f5-40d1-92cf-a19fb24f6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_valid, X_test, y_train, y_valid, y_test, output_file='testresults.json'):\n",
    "    # Import numpy if not already imported\n",
    "    import numpy as np\n",
    "    \n",
    "    # Scale the features for better model performance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Dictionary to store performance metrics\n",
    "    performance_metrics = {}\n",
    "    \n",
    "    # ---- Improved SVM Model ----\n",
    "    print(\"[INFO] Training improved SVM model...\")\n",
    "    \n",
    "    # Use a different kernel (linear instead of rbf)\n",
    "    # Linear kernels often work better for time-series financial data with many features\n",
    "    svm_model = SVC(kernel='linear', probability=True, C=1.0, class_weight='balanced')\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    svm_valid_pred = svm_model.predict(X_valid_scaled)\n",
    "    svm_valid_acc = accuracy_score(y_valid, svm_valid_pred)\n",
    "    print(f\"SVM Validation Accuracy: {svm_valid_acc:.4f}\")\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    y_test_pred_svm = svm_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Store SVM metrics\n",
    "    performance_metrics['svm'] = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred_svm),\n",
    "        'precision': precision_score(y_test, y_test_pred_svm),\n",
    "        'recall': recall_score(y_test, y_test_pred_svm),\n",
    "        'f1_score': f1_score(y_test, y_test_pred_svm),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_test_pred_svm).tolist()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSVM Model Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_test_pred_svm))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_test_pred_svm))\n",
    "    print(f\"Accuracy: {performance_metrics['svm']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {performance_metrics['svm']['precision']:.4f}\")\n",
    "    print(f\"Recall: {performance_metrics['svm']['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {performance_metrics['svm']['f1_score']:.4f}\")\n",
    "    \n",
    "    # ---- Random Forest Model ----\n",
    "    print(\"\\n[INFO] Training Random Forest model...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    rf_valid_pred = rf_model.predict(X_valid)\n",
    "    rf_valid_acc = accuracy_score(y_valid, rf_valid_pred)\n",
    "    print(f\"Random Forest Validation Accuracy: {rf_valid_acc:.4f}\")\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    y_test_pred_rf = rf_model.predict(X_test)\n",
    "    \n",
    "    # Store RF metrics\n",
    "    performance_metrics['random_forest'] = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred_rf),\n",
    "        'precision': precision_score(y_test, y_test_pred_rf),\n",
    "        'recall': recall_score(y_test, y_test_pred_rf),\n",
    "        'f1_score': f1_score(y_test, y_test_pred_rf),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_test_pred_rf).tolist()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nRandom Forest Model Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_test_pred_rf))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_test_pred_rf))\n",
    "    print(f\"Accuracy: {performance_metrics['random_forest']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {performance_metrics['random_forest']['precision']:.4f}\")\n",
    "    print(f\"Recall: {performance_metrics['random_forest']['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {performance_metrics['random_forest']['f1_score']:.4f}\")\n",
    "    \n",
    "    # ---- XGBoost Model ----\n",
    "    print(\"\\n[INFO] Training tuned XGBoost model...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),  # Adjust for class imbalance\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    xgb_valid_pred = xgb_model.predict(X_valid)\n",
    "    xgb_valid_acc = accuracy_score(y_valid, xgb_valid_pred)\n",
    "    print(f\"XGBoost Validation Accuracy: {xgb_valid_acc:.4f}\")\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Store XGBoost metrics\n",
    "    performance_metrics['xgboost'] = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred_xgb),\n",
    "        'precision': precision_score(y_test, y_test_pred_xgb),\n",
    "        'recall': recall_score(y_test, y_test_pred_xgb),\n",
    "        'f1_score': f1_score(y_test, y_test_pred_xgb),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_test_pred_xgb).tolist()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nXGBoost Model Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_test_pred_xgb))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_test_pred_xgb))\n",
    "    print(f\"Accuracy: {performance_metrics['xgboost']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {performance_metrics['xgboost']['precision']:.4f}\")\n",
    "    print(f\"Recall: {performance_metrics['xgboost']['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {performance_metrics['xgboost']['f1_score']:.4f}\")\n",
    "    \n",
    "    # ---- Ensemble approach ----\n",
    "    print(\"\\n[INFO] Creating ensemble prediction...\")\n",
    "    \n",
    "    # Create ensemble prediction (majority voting)\n",
    "    ensemble_pred = np.zeros(len(y_test))\n",
    "    \n",
    "    # Get probability predictions\n",
    "    svm_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    rf_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Weighted average of probabilities (giving more weight to the best performing model)\n",
    "    # Check which model had best validation accuracy and weight accordingly\n",
    "    best_val_acc = max(svm_valid_acc, rf_valid_acc, xgb_valid_acc)\n",
    "    \n",
    "    if best_val_acc == svm_valid_acc:\n",
    "        weights = [0.5, 0.25, 0.25]  # SVM gets more weight\n",
    "    elif best_val_acc == rf_valid_acc:\n",
    "        weights = [0.25, 0.5, 0.25]  # Random Forest gets more weight\n",
    "    else:\n",
    "        weights = [0.25, 0.25, 0.5]  # XGBoost gets more weight\n",
    "    \n",
    "    # Calculate weighted probabilities\n",
    "    ensemble_proba = (\n",
    "        weights[0] * svm_proba + \n",
    "        weights[1] * rf_proba + \n",
    "        weights[2] * xgb_proba\n",
    "    )\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Store ensemble metrics\n",
    "    performance_metrics['ensemble'] = {\n",
    "        'accuracy': accuracy_score(y_test, ensemble_pred),\n",
    "        'precision': precision_score(y_test, ensemble_pred),\n",
    "        'recall': recall_score(y_test, ensemble_pred),\n",
    "        'f1_score': f1_score(y_test, ensemble_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, ensemble_pred).tolist(),\n",
    "        'weights': weights\n",
    "    }\n",
    "    \n",
    "    print(\"\\nEnsemble Model Test Set Performance:\")\n",
    "    print(classification_report(y_test, ensemble_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, ensemble_pred))\n",
    "    print(f\"Accuracy: {performance_metrics['ensemble']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {performance_metrics['ensemble']['precision']:.4f}\")\n",
    "    print(f\"Recall: {performance_metrics['ensemble']['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {performance_metrics['ensemble']['f1_score']:.4f}\")\n",
    "    \n",
    "    # Extract feature importances from Random Forest\n",
    "    feature_importances = None\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        print(\"\\nRandom Forest Feature Importance:\")\n",
    "        feature_importances = {}\n",
    "        for feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n",
    "            feature_importances[feature] = importance\n",
    "            print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Save results to file\n",
    "    save_results_to_file(\n",
    "        models={'svm': svm_model, 'rf': rf_model, 'xgb': xgb_model},\n",
    "        performance_metrics=performance_metrics,\n",
    "        feature_importances=feature_importances,\n",
    "        filename=output_file\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'svm_model': svm_model, \n",
    "        'rf_model': rf_model, \n",
    "        'xgb_model': xgb_model,\n",
    "        'ensemble_weights': weights,\n",
    "        'scaler': scaler,\n",
    "        'performance_metrics': performance_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb39247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(self, models, performance_metrics, feature_importances=None, filename='testresults.json'):\n",
    "    \"\"\"\n",
    "    Save model evaluation results to an external file.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary containing trained models\n",
    "        performance_metrics: Dictionary containing accuracy, precision, recall, and f1 scores\n",
    "        feature_importances: Optional dictionary containing feature importance\n",
    "        filename: Name of the output file (default is 'testresults.json')\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert numpy arrays and other non-serializable objects to lists\n",
    "    def serialize_item(item):\n",
    "        if isinstance(item, np.ndarray):\n",
    "            return item.tolist()\n",
    "        elif isinstance(item, np.float32) or isinstance(item, np.float64):\n",
    "            return float(item)\n",
    "        elif isinstance(item, np.int32) or isinstance(item, np.int64):\n",
    "            return int(item)\n",
    "        else:\n",
    "            return item\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'performance_metrics': performance_metrics\n",
    "    }\n",
    "    \n",
    "    # Add feature importances if available\n",
    "    if feature_importances:\n",
    "        results['feature_importances'] = {k: serialize_item(v) for k, v in feature_importances.items()}\n",
    "    \n",
    "    # Determine file extension and save accordingly\n",
    "    file_ext = filename.split('.')[-1].lower()\n",
    "    \n",
    "    if file_ext == 'json':\n",
    "        # Save as JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "    elif file_ext in ['csv', 'txt']:\n",
    "        # Save as CSV or text file\n",
    "        import csv\n",
    "        with open(filename, 'w', newline='') as f:\n",
    "            if file_ext == 'csv':\n",
    "                writer = csv.writer(f)\n",
    "                # Write headers\n",
    "                writer.writerow(['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "                # Write data\n",
    "                for model, metrics in performance_metrics.items():\n",
    "                    writer.writerow([model, metrics['accuracy'], metrics['precision'], \n",
    "                                    metrics['recall'], metrics['f1_score']])\n",
    "            else:  # text file\n",
    "                f.write(f\"Model Evaluation Results - {results['timestamp']}\\n\\n\")\n",
    "                for model, metrics in performance_metrics.items():\n",
    "                    f.write(f\"Model: {model}\\n\")\n",
    "                    f.write(f\"Accuracy: {metrics['accuracy']}\\n\")\n",
    "                    f.write(f\"Precision: {metrics['precision']}\\n\")\n",
    "                    f.write(f\"Recall: {metrics['recall']}\\n\")\n",
    "                    f.write(f\"F1 Score: {metrics['f1_score']}\\n\\n\")\n",
    "                \n",
    "                if feature_importances:\n",
    "                    f.write(\"Feature Importances:\\n\")\n",
    "                    for feature, importance in feature_importances.items():\n",
    "                        f.write(f\"{feature}: {importance}\\n\")\n",
    "    else:\n",
    "        # Default to pickle for other extensions\n",
    "        import pickle\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"[INFO] Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7aa4b23-6ca6-441b-8efd-98765fcac73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    crypto_folder = 'data/'  # update this\n",
    "    reddit_folder = 'data/'   # update this\n",
    "    output_file = 'testresults.json'  # you can change to .csv, .txt, or .pkl\n",
    "\n",
    "    print(\"[INFO] Loading crypto data...\")\n",
    "    crypto_df = unzip_and_load_crypto(crypto_folder)\n",
    "\n",
    "    print(\"[INFO] Loading reddit sentiment data...\")\n",
    "    reddit_df = load_reddit_sentiment(reddit_folder)\n",
    "\n",
    "    print(\"[INFO] Preprocessing data...\")\n",
    "    data = preprocess_data(crypto_df, reddit_df)\n",
    "\n",
    "    print(\"[INFO] Creating lagged features...\")\n",
    "    data = create_lagged_features(data, max_lag_hours=12)\n",
    "\n",
    "    print(\"[INFO] Splitting data...\")\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = split_data(data)\n",
    "\n",
    "    print(\"[INFO] Training and evaluating model...\")\n",
    "    model = train_and_evaluate(X_train, X_valid, X_test, y_train, y_valid, y_test, output_file)\n",
    "\n",
    "    print(\"[INFO] Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9e1f870-923d-4257-90e3-628e03421f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading crypto data...\n",
      "[INFO] Loading reddit sentiment data...\n",
      "[INFO] Preprocessing data...\n",
      "[INFO] Creating lagged features...\n",
      "[INFO] Splitting data...\n",
      "[INFO] Training and evaluating model...\n",
      "[INFO] Training improved SVM model...\n",
      "SVM Validation Accuracy: 0.6111\n",
      "\n",
      "SVM Model Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.41      0.55        51\n",
      "           1       0.36      0.77      0.49        22\n",
      "\n",
      "    accuracy                           0.52        73\n",
      "   macro avg       0.58      0.59      0.52        73\n",
      "weighted avg       0.67      0.52      0.53        73\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21 30]\n",
      " [ 5 17]]\n",
      "Accuracy: 0.5205\n",
      "Precision: 0.3617\n",
      "Recall: 0.7727\n",
      "F1 Score: 0.4928\n",
      "\n",
      "[INFO] Training Random Forest model...\n",
      "Random Forest Validation Accuracy: 0.5278\n",
      "\n",
      "Random Forest Model Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.22      0.34        51\n",
      "           1       0.32      0.86      0.47        22\n",
      "\n",
      "    accuracy                           0.41        73\n",
      "   macro avg       0.55      0.54      0.40        73\n",
      "weighted avg       0.65      0.41      0.38        73\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11 40]\n",
      " [ 3 19]]\n",
      "Accuracy: 0.4110\n",
      "Precision: 0.3220\n",
      "Recall: 0.8636\n",
      "F1 Score: 0.4691\n",
      "\n",
      "[INFO] Training tuned XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j0e/.local/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [03:29:18] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Accuracy: 0.6111\n",
      "\n",
      "XGBoost Model Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.37      0.50        51\n",
      "           1       0.33      0.73      0.46        22\n",
      "\n",
      "    accuracy                           0.48        73\n",
      "   macro avg       0.55      0.55      0.48        73\n",
      "weighted avg       0.63      0.48      0.49        73\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19 32]\n",
      " [ 6 16]]\n",
      "Accuracy: 0.4795\n",
      "Precision: 0.3333\n",
      "Recall: 0.7273\n",
      "F1 Score: 0.4571\n",
      "\n",
      "[INFO] Creating ensemble prediction...\n",
      "\n",
      "Ensemble Model Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.29      0.43        51\n",
      "           1       0.35      0.86      0.49        22\n",
      "\n",
      "    accuracy                           0.47        73\n",
      "   macro avg       0.59      0.58      0.46        73\n",
      "weighted avg       0.69      0.47      0.45        73\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15 36]\n",
      " [ 3 19]]\n",
      "Accuracy: 0.4658\n",
      "Precision: 0.3455\n",
      "Recall: 0.8636\n",
      "F1 Score: 0.4935\n",
      "\n",
      "Random Forest Feature Importance:\n",
      "sentiment: 0.0207\n",
      "sentiment_prev_1h: 0.0186\n",
      "close_price_prev_1h: 0.1168\n",
      "sentiment_prev_2h: 0.0212\n",
      "close_price_prev_2h: 0.0909\n",
      "sentiment_prev_3h: 0.0207\n",
      "close_price_prev_3h: 0.0614\n",
      "sentiment_prev_4h: 0.0192\n",
      "close_price_prev_4h: 0.0342\n",
      "sentiment_prev_5h: 0.0235\n",
      "close_price_prev_5h: 0.0319\n",
      "sentiment_prev_6h: 0.0228\n",
      "close_price_prev_6h: 0.0357\n",
      "sentiment_prev_7h: 0.0385\n",
      "close_price_prev_7h: 0.0401\n",
      "sentiment_prev_8h: 0.0230\n",
      "close_price_prev_8h: 0.0350\n",
      "sentiment_prev_9h: 0.0284\n",
      "close_price_prev_9h: 0.0509\n",
      "sentiment_prev_10h: 0.0242\n",
      "close_price_prev_10h: 0.0537\n",
      "sentiment_prev_11h: 0.0143\n",
      "close_price_prev_11h: 0.0628\n",
      "sentiment_prev_12h: 0.0291\n",
      "close_price_prev_12h: 0.0822\n",
      "[INFO] Results saved to testresults.json\n",
      "[INFO] Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6dfe7-b980-4050-84cb-336c8d67b7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
